{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "23sr8I8OE_-b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import scipy.sparse as sp\n",
        "import pickle as pkl\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n86LR0fFGH0Y"
      },
      "outputs": [],
      "source": [
        "def normalized_adj(adj):\n",
        "    \"\"\"\n",
        "    Row-normalize matrix:  A_norm = D^{-1/2} * A * D^{-1/2}.\n",
        "    \"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    norm_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "    return norm_adj.astype(np.float32)\n",
        "\n",
        "def sparse_to_tensor(sparse_mx):\n",
        "    \"\"\"\n",
        "    Convert a scipy sparse matrix (COO) to a TF2 SparseTensor.\n",
        "    \"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo()\n",
        "    indices = np.column_stack((sparse_mx.row, sparse_mx.col))\n",
        "    return tf.sparse.SparseTensor(\n",
        "        indices=indices,\n",
        "        values=sparse_mx.data,\n",
        "        dense_shape=sparse_mx.shape\n",
        "    )\n",
        "\n",
        "def calculate_laplacian(adj):\n",
        "    \"\"\"\n",
        "    Computes the normalized adjacency + identity, then returns a tf.sparse.SparseTensor.\n",
        "    \"\"\"\n",
        "    # Add self-loops\n",
        "    adj_normalized = normalized_adj(adj + sp.eye(adj.shape[0]))\n",
        "    # Convert to SparseTensor\n",
        "    return sparse_to_tensor(sp.csr_matrix(adj_normalized))\n",
        "\n",
        "def weight_variable_glorot(input_dim, output_dim):\n",
        "    \"\"\"\n",
        "    Used in the old GCN code. For TF2, you can simply use keras initializers directly.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initializer = tf.random_uniform_initializer(minval=-init_range, maxval=init_range)\n",
        "    return tf.Variable(initializer(shape=(input_dim, output_dim)), dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00mzQceUGRDH"
      },
      "outputs": [],
      "source": [
        "def load_sz_data():\n",
        "    \"\"\"\n",
        "    Loads the Shenzhen dataset (adjacency and speed).\n",
        "    Expects 'sz_adj.csv' and 'sz_speed.csv' in the current folder.\n",
        "    \"\"\"\n",
        "    sz_adj = pd.read_csv('data/sz_adj.csv', header=None)\n",
        "    adj = np.asmatrix(sz_adj)\n",
        "    sz_tf = pd.read_csv('data/sz_speed.csv')\n",
        "    return sz_tf, adj\n",
        "\n",
        "def load_los_data():\n",
        "    \"\"\"\n",
        "    Loads the Los Loop dataset (adjacency and speed).\n",
        "    Expects 'los_adj.csv' and 'los_speed.csv' in the current folder.\n",
        "    \"\"\"\n",
        "    los_adj = pd.read_csv('data/los_adj.csv', header=None)\n",
        "    adj = np.asmatrix(los_adj)\n",
        "    los_tf = pd.read_csv('data/los_speed.csv')\n",
        "    return los_tf, adj\n",
        "\n",
        "def preprocess_data(data, time_len, train_rate, seq_len, pre_len):\n",
        "    \"\"\"\n",
        "    Generate trainX, trainY, testX, testY from the time series data.\n",
        "    data: shape (time_len, num_nodes)\n",
        "    \"\"\"\n",
        "    train_size = int(time_len * train_rate)\n",
        "    train_data = data[0:train_size]\n",
        "    test_data = data[train_size:time_len]\n",
        "    trainX, trainY = [], []\n",
        "    testX, testY = [], []\n",
        "    # Build training samples\n",
        "    for i in range(len(train_data) - seq_len - pre_len):\n",
        "        a = train_data[i: i + seq_len + pre_len]\n",
        "        trainX.append(a[0: seq_len])\n",
        "        trainY.append(a[seq_len: seq_len + pre_len])\n",
        "    # Build testing samples\n",
        "    for i in range(len(test_data) - seq_len - pre_len):\n",
        "        b = test_data[i: i + seq_len + pre_len]\n",
        "        testX.append(b[0: seq_len])\n",
        "        testY.append(b[seq_len: seq_len + pre_len])\n",
        "    trainX = np.array(trainX, dtype=np.float32)\n",
        "    trainY = np.array(trainY, dtype=np.float32)\n",
        "    testX = np.array(testX, dtype=np.float32)\n",
        "    testY = np.array(testY, dtype=np.float32)\n",
        "    return trainX, trainY, testX, testY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ywgz47xHGWAX"
      },
      "outputs": [],
      "source": [
        "def plot_result(test_result, test_label, path):\n",
        "    \"\"\"\n",
        "    Visualize the first sensor's prediction vs. ground truth across all time,\n",
        "    and also for a single day.\n",
        "    test_result, test_label: arrays of shape (time, num_nodes) or (time,).\n",
        "    \"\"\"\n",
        "    fig1 = plt.figure(figsize=(7, 1.5))\n",
        "    a_pred = test_result[:, 0]\n",
        "    a_true = test_label[:, 0]\n",
        "    plt.plot(a_pred, 'r-', label='prediction')\n",
        "    plt.plot(a_true, 'b-', label='true')\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.savefig(path + '/test_all.jpg')\n",
        "    plt.show()\n",
        "    # One day example (e.g., first 96 points)\n",
        "    fig2 = plt.figure(figsize=(7, 1.5))\n",
        "    a_pred_day = a_pred[:96]\n",
        "    a_true_day = a_true[:96]\n",
        "    plt.plot(a_pred_day, 'r-', label='prediction')\n",
        "    plt.plot(a_true_day, 'b-', label='true')\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.savefig(path + '/test_oneday.jpg')\n",
        "    plt.show()\n",
        "\n",
        "def plot_error(train_rmse, train_loss, test_rmse, test_acc, test_mae, path):\n",
        "    \"\"\"Plots the metrics over epochs.\"\"\"\n",
        "    fig1 = plt.figure(figsize=(5,3))\n",
        "    plt.plot(train_rmse, 'r-', label='train_rmse')\n",
        "    plt.plot(test_rmse, 'b-', label='test_rmse')\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.savefig(path + '/rmse.jpg')\n",
        "    plt.show()\n",
        "    fig2 = plt.figure(figsize=(5,3))\n",
        "    plt.plot(train_loss, 'b-', label='train_loss')\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.savefig(path + '/train_loss.jpg')\n",
        "    plt.show()\n",
        "    fig3 = plt.figure(figsize=(5,3))\n",
        "    plt.plot(train_rmse, 'b-', label='train_rmse')\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.savefig(path + '/train_rmse.jpg')\n",
        "    plt.show()\n",
        "    fig4 = plt.figure(figsize=(5,3))\n",
        "    plt.plot(test_acc, 'b-', label='test_acc')\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.savefig(path + '/test_acc.jpg')\n",
        "    plt.show()\n",
        "    fig5 = plt.figure(figsize=(5,3))\n",
        "    plt.plot(test_rmse, 'b-', label='test_rmse')\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.savefig(path + '/test_rmse.jpg')\n",
        "    plt.show()\n",
        "    fig6 = plt.figure(figsize=(5,3))\n",
        "    plt.plot(test_mae, 'b-', label='test_mae')\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.savefig(path + '/test_mae.jpg')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nngbruhpGYsD"
      },
      "outputs": [],
      "source": [
        "class TGCNCell(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Temporal Graph Convolutional Cell:\n",
        "    GRU-like gating + graph convolution on [batch, num_nodes, features].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_units, adj, num_nodes, activation=tf.nn.tanh, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_units = num_units\n",
        "        self.num_nodes = num_nodes\n",
        "        self.activation = activation\n",
        "        # Precompute sparse Laplacian\n",
        "        self.adj = calculate_laplacian(adj)  # tf.sparse.SparseTensor\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        # Flattened hidden state: (batch, num_nodes * num_units)\n",
        "        return self.num_nodes * self.num_units\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        # We'll output the same shape as the state per time step\n",
        "        return self.num_nodes * self.num_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        input_shape: (batch, num_nodes) per time step.\n",
        "        We'll create parameters for GRU gates and candidate.\n",
        "        \"\"\"\n",
        "        # Gate parameters (r, u)\n",
        "        self.w_gates = self.add_weight(\n",
        "            shape=(1 + self.num_units, 2 * self.num_units),\n",
        "            initializer='glorot_uniform',\n",
        "            name='w_gates'\n",
        "        )\n",
        "        self.b_gates = self.add_weight(\n",
        "            shape=(2 * self.num_units,),\n",
        "            initializer='zeros',\n",
        "            name='b_gates'\n",
        "        )\n",
        "        # Candidate parameters\n",
        "        self.w_cand = self.add_weight(\n",
        "            shape=(1 + self.num_units, self.num_units),\n",
        "            initializer='glorot_uniform',\n",
        "            name='w_cand'\n",
        "        )\n",
        "        self.b_cand = self.add_weight(\n",
        "            shape=(self.num_units,),\n",
        "            initializer='zeros',\n",
        "            name='b_cand'\n",
        "        )\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        \"\"\"\n",
        "        inputs: (batch, num_nodes)\n",
        "        states: list with one element = previous_state => (batch, num_nodes * num_units)\n",
        "        returns: output, new_state\n",
        "        \"\"\"\n",
        "        prev_state = states[0]  # shape: (batch, num_nodes * num_units)\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        # Expand input => (batch, num_nodes, 1)\n",
        "        x_in = tf.expand_dims(inputs, axis=-1)\n",
        "        # Reshape state => (batch, num_nodes, num_units)\n",
        "        h_prev = tf.reshape(prev_state, [batch_size, self.num_nodes, self.num_units])\n",
        "        # ---- GATES ----\n",
        "        # Concat => shape (batch, num_nodes, 1 + num_units)\n",
        "        gates_input = tf.concat([x_in, h_prev], axis=2)\n",
        "        gates = self._gconv(gates_input, self.w_gates, self.b_gates)\n",
        "        gates = tf.sigmoid(gates)  # (batch, num_nodes, 2*num_units)\n",
        "        r, u = tf.split(gates, 2, axis=-1)  # each => (batch, num_nodes, num_units)\n",
        "        # ---- CANDIDATE ----\n",
        "        r_state = r * h_prev\n",
        "        cand_input = tf.concat([x_in, r_state], axis=2)\n",
        "        cand = self._gconv(cand_input, self.w_cand, self.b_cand)\n",
        "        if self.activation is not None:\n",
        "            cand = self.activation(cand)\n",
        "        # new hidden\n",
        "        new_h = u * h_prev + (1.0 - u) * cand\n",
        "        # Flatten => (batch, num_nodes * num_units)\n",
        "        new_h_flat = tf.reshape(new_h, [batch_size, self.num_nodes * self.num_units])\n",
        "        return new_h_flat, [new_h_flat]\n",
        "\n",
        "    def _gconv(self, x_s, w, b):\n",
        "        \"\"\"\n",
        "        Graph convolution step: multiply by adjacency, then apply w, b.\n",
        "         - x_s: (batch, num_nodes, in_features)\n",
        "         - w: shape (in_features, out_features)\n",
        "         - b: shape (out_features,)\n",
        "        Return shape: (batch, num_nodes, out_features)\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(x_s)[0]\n",
        "        in_features = tf.shape(x_s)[2]\n",
        "        # => (num_nodes, in_features, batch)\n",
        "        x_t = tf.transpose(x_s, [1, 2, 0])\n",
        "        # => (num_nodes, in_features*batch)\n",
        "        x_t = tf.reshape(x_t, [self.num_nodes, -1])\n",
        "        # adjacency multiplication\n",
        "        x_adj = tf.sparse.sparse_dense_matmul(self.adj, x_t)  # => (num_nodes, in_features*batch)\n",
        "        # => (num_nodes, in_features, batch)\n",
        "        x_adj = tf.reshape(x_adj, [self.num_nodes, in_features, batch_size])\n",
        "        # => (batch, num_nodes, in_features)\n",
        "        x_adj = tf.transpose(x_adj, [2, 0, 1])\n",
        "        # flatten => (batch*num_nodes, in_features)\n",
        "        x_adj = tf.reshape(x_adj, [batch_size * self.num_nodes, in_features])\n",
        "        out = tf.matmul(x_adj, w) + b  # => (batch*num_nodes, out_features)\n",
        "        out_features = tf.shape(out)[1]\n",
        "        # => (batch, num_nodes, out_features)\n",
        "        out = tf.reshape(out, [batch_size, self.num_nodes, out_features])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QrhKdcMGeoC",
        "outputId": "57c7f9ac-5bd2-4a29-dbb6-afc5ce1f0007"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Hyperparameters\n",
        "################################################################################\n",
        "learning_rate = 0.001\n",
        "training_epoch = 10\n",
        "gru_units = 64\n",
        "seq_len = 12\n",
        "pre_len = 3\n",
        "train_rate = 0.8\n",
        "batch_size = 32\n",
        "dataset = 'los'  # or 'sz'\n",
        "model_name = 'tgcn'\n",
        "lambda_loss = 0.0015  # L2 factor\n",
        "################################################################################\n",
        "# Load Data\n",
        "################################################################################\n",
        "if dataset == 'sz':\n",
        "    data, adj = load_sz_data()\n",
        "elif dataset == 'los':\n",
        "    data, adj = load_los_data()\n",
        "else:\n",
        "    raise ValueError(\"Unknown dataset\")\n",
        "data = np.array(data, dtype=np.float32)\n",
        "time_len = data.shape[0]\n",
        "num_nodes = data.shape[1]\n",
        "max_value = data.max()\n",
        "data = data / max_value\n",
        "trainX, trainY, testX, testY = preprocess_data(data, time_len, train_rate, seq_len, pre_len)\n",
        "################################################################################\n",
        "# TGCN Keras Model: RNN with TGCNCell\n",
        "################################################################################\n",
        "class TGCNModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Wrap an RNN layer using our TGCNCell, plus a final Dense to predict pre_len steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_units, adj, num_nodes, pre_len):\n",
        "        super().__init__()\n",
        "        self.num_units = num_units\n",
        "        self.num_nodes = num_nodes\n",
        "        self.pre_len = pre_len\n",
        "        # Our TGCNCell\n",
        "        self.cell = TGCNCell(num_units, adj, num_nodes)\n",
        "        # RNN: unroll over seq_len\n",
        "        self.rnn = tf.keras.layers.RNN(self.cell, return_sequences=True)\n",
        "        # Final Dense to map from (num_nodes*units) -> (num_nodes*pre_len)\n",
        "        self.out_dense = tf.keras.layers.Dense(self.num_nodes * self.pre_len)\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"\n",
        "        inputs shape: (batch, seq_len, num_nodes)\n",
        "        Return shape: (batch*pre_len, num_nodes)\n",
        "        \"\"\"\n",
        "        # 1) RNN => (batch, seq_len, num_nodes*units)\n",
        "        rnn_outputs = self.rnn(inputs, training=training)\n",
        "        # 2) Take final time step => (batch, num_nodes*units)\n",
        "        last_output = rnn_outputs[:, -1, :]\n",
        "        # 3) Dense => (batch, num_nodes*pre_len)\n",
        "        out = self.out_dense(last_output)\n",
        "        # 4) Reshape => (batch, pre_len, num_nodes)\n",
        "        out = tf.reshape(out, [-1, self.pre_len, self.num_nodes])\n",
        "        # => (batch, num_nodes, pre_len)\n",
        "        out = tf.transpose(out, [0, 2, 1])\n",
        "        # => (batch*pre_len, num_nodes)\n",
        "        out = tf.reshape(out, [-1, self.num_nodes])\n",
        "        return out\n",
        "model = TGCNModel(gru_units, adj, num_nodes, pre_len)\n",
        "################################################################################\n",
        "# Prepare tf.data Datasets\n",
        "################################################################################\n",
        "train_size = trainX.shape[0]\n",
        "test_size = testX.shape[0]\n",
        "def gen_train():\n",
        "    for i in range(train_size):\n",
        "        yield trainX[i], trainY[i]\n",
        "def gen_test():\n",
        "    for i in range(test_size):\n",
        "        yield testX[i], testY[i]\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    gen_train,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=((seq_len, num_nodes), (pre_len, num_nodes))\n",
        ").batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    gen_test,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=((seq_len, num_nodes), (pre_len, num_nodes))\n",
        ").batch(test_size)  # one batch for entire test, or break it up as you like\n",
        "################################################################################\n",
        "# Loss & Optimizer\n",
        "################################################################################\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "def loss_fn(pred, labels, model_vars):\n",
        "    # pred, labels => (batch*pre_len, num_nodes)\n",
        "    mse_loss = tf.reduce_mean(tf.square(pred - labels))\n",
        "\n",
        "    # L2 regularization\n",
        "    l2_reg = 0.0\n",
        "    for v in model_vars:\n",
        "        l2_reg += tf.nn.l2_loss(v)\n",
        "    return mse_loss + lambda_loss * l2_reg\n",
        "def evaluation(a, b):\n",
        "    # a, b: np arrays\n",
        "    rmse = math.sqrt(mean_squared_error(a, b))\n",
        "    mae = mean_absolute_error(a, b)\n",
        "    F_norm = la.norm(a - b, 'fro') / la.norm(a, 'fro')\n",
        "    r2 = 1 - ((a - b)**2).sum() / ((a - a.mean())**2).sum()\n",
        "    var = 1 - (np.var(a - b) / np.var(a))\n",
        "    return rmse, mae, 1 - F_norm, r2, var\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"tgcn_model_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"tgcn_model_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ tgcn_cell_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TGCNCell</span>)          │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,672</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rnn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RNN</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13248</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,672</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">621</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,227,629</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ tgcn_cell_3 (\u001b[38;5;33mTGCNCell\u001b[0m)          │ ?                      │        \u001b[38;5;34m12,672\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rnn_3 (\u001b[38;5;33mRNN\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m13248\u001b[0m)      │        \u001b[38;5;34m12,672\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m621\u001b[0m)            │     \u001b[38;5;34m8,227,629\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,240,301</span> (31.43 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,240,301\u001b[0m (31.43 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,240,301</span> (31.43 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,240,301\u001b[0m (31.43 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">207</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ tgcn_model_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TGCNModel</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">207</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,240,301</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m207\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ tgcn_model_3 (\u001b[38;5;33mTGCNModel\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m207\u001b[0m)            │     \u001b[38;5;34m8,240,301\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,240,301</span> (31.43 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,240,301\u001b[0m (31.43 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,240,301</span> (31.43 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,240,301\u001b[0m (31.43 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# تعریف شکل ورودی\n",
        "input_shape = (seq_len, num_nodes)  # (طول دنباله زمانی، تعداد گره‌ها)\n",
        "\n",
        "# تعریف لایه Input\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "# ایجاد مدل TGCN\n",
        "model = TGCNModel(gru_units, adj, num_nodes, pre_len)\n",
        "\n",
        "# اتصال ورودی به مدل\n",
        "outputs = model(inputs)\n",
        "model.summary()\n",
        "# ایجاد مدل Keras با ورودی و خروجی\n",
        "tgcn_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# نمایش خلاصه مدل\n",
        "tgcn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Training Loop\n",
        "################################################################################\n",
        "train_rmse_list, train_loss_list = [], []\n",
        "test_rmse_list, test_mae_list, test_acc_list, test_loss_list = [], [], [], []\n",
        "t_start = time.time()\n",
        "for epoch in range(training_epoch):\n",
        "    # ---------- Train ----------\n",
        "    epoch_loss = 0.0\n",
        "    epoch_rmse = 0.0\n",
        "    count_batch = 0\n",
        "\n",
        "    for batch_x, batch_y in train_dataset:\n",
        "        # batch_x => (batch, seq_len, num_nodes)\n",
        "        # batch_y => (batch, pre_len, num_nodes)\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(batch_x, training=True)  # => (batch*pre_len, num_nodes)\n",
        "            y_true = tf.reshape(batch_y, [-1, num_nodes])\n",
        "            l = loss_fn(y_pred, y_true, model.trainable_variables)\n",
        "\n",
        "        grads = tape.gradient(l, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        epoch_loss += l.numpy()\n",
        "        batch_rmse_val = tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true))) * max_value\n",
        "        epoch_rmse += batch_rmse_val.numpy()\n",
        "        count_batch += 1\n",
        "\n",
        "    epoch_loss /= count_batch\n",
        "    epoch_rmse /= count_batch\n",
        "    train_loss_list.append(epoch_loss)\n",
        "    train_rmse_list.append(epoch_rmse)\n",
        "\n",
        "    # ---------- Test ----------\n",
        "    for test_x, test_y in test_dataset:\n",
        "        y_pred_test = model(test_x, training=False)\n",
        "        y_true_test = tf.reshape(test_y, [-1, num_nodes])\n",
        "        loss_test = loss_fn(y_pred_test, y_true_test, model.trainable_variables).numpy()\n",
        "\n",
        "        pred_np = (y_pred_test.numpy()) * max_value\n",
        "        true_np = (y_true_test.numpy()) * max_value\n",
        "        rmse, mae, acc, r2, var = evaluation(true_np, pred_np)\n",
        "\n",
        "        test_loss_list.append(loss_test)\n",
        "        test_rmse_list.append(rmse)\n",
        "        test_mae_list.append(mae)\n",
        "        test_acc_list.append(acc)\n",
        "        break  # end test loop\n",
        "\n",
        "    print(f\"Epoch {epoch}, Train Loss={epoch_loss:.4f}, Train RMSE={epoch_rmse:.2f}, \"\n",
        "          f\"Test Loss={loss_test:.4f}, Test RMSE={rmse:.2f}, Test Acc={acc:.4f}\")\n",
        "t_end = time.time()\n",
        "print(\"Total Training Time: %.2f s\" % (t_end - t_start))\n",
        "################################################################################\n",
        "# Best epoch\n",
        "################################################################################\n",
        "best_idx = int(np.argmin(test_rmse_list))\n",
        "print(\"Best epoch:\", best_idx,\n",
        "      \"Test RMSE:\", test_rmse_list[best_idx],\n",
        "      \"Test MAE:\", test_mae_list[best_idx],\n",
        "      \"Test Acc:\", test_acc_list[best_idx])\n",
        "################################################################################\n",
        "# Save model\n",
        "################################################################################\n",
        "os.makedirs(\"out\", exist_ok=True)\n",
        "model.save_weights(\"out/tgcn_model_weights.weights.h5\")\n",
        "print(\"Model weights saved.\")\n",
        "\n",
        "# Example retrieving final predictions\n",
        "for test_x, test_y in test_dataset:\n",
        "    final_preds = model(test_x, training=False).numpy() * max_value\n",
        "    final_truth = test_y.numpy().reshape([-1, num_nodes]) * max_value\n",
        "    break\n",
        "np.savetxt(\"out/test_pred.csv\", final_preds, delimiter=\",\")\n",
        "np.savetxt(\"out/test_true.csv\", final_truth, delimiter=\",\")\n",
        "print(\"Saved final predictions to CSV.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
